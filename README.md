# Generative-info
This is a repository about Generative technical, mianly includes papers and projects of GAN, VAE, and Diffusion model. Please remember, this repository was created just for myself stduy, please give me a star if you find some help in this hub. We will continue update....

------------

- [GAN](#gan)
- [Diffusion Model](#diffusion)
- [VAE](#vae)

## GAN

|   |   |   |   |
| :------------: | :------------: | :------------: | :------------: |
| ICLR2022 | [Vector-quantized Image Modeling with Improved VQGAN](https://arxiv.org/abs/2110.04627) | None | Basic |



## Diffusion

|   |   |   |   |
| :------------: | :------------: | :------------: | :------------: |
| arxiv24.08 | [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528) | [code](https://github.com/showlab/show-o) | Image/Video Edit & Text2Image/Video |
| CVPR2022 | [Diffusionclip: Text-guided diffusion models for robust image manipulation](https://arxiv.org/abs/2110.02711) | [code](https://github.com/gwang-kim/DiffusionCLIP) |  Image/Video Edit |
| CVPR2022 | [Blended Diffusion for Text-driven Editing of Natural Images](https://arxiv.org/abs/2111.14818) | [code](https://github.com/omriav/blended-diffusion) |  Image/Video Edit |
| arxiv24.08 | [ControlNeXt: Powerful and Efficient Control for Image and Video Generation](https://arxiv.org/abs/2408.06070) | [code](https://github.com/dvlab-research/ControlNeXt) |  Image/Video Edit |
| arxiv24.06 | [UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation](https://arxiv.org/abs/2406.01188) | [code](https://github.com/ali-vilab/UniAnimate) |  Image/Video Edit |
| ICLR2024 | [PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models](https://arxiv.org/abs/2401.05252) | [code](https://github.com/PixArt-alpha/PixArt-alpha) |  Image/Video Edit |
| CVPR2024 | [DreamVideo: Composing Your Dream Videos with Customized Subject and Motion](https://arxiv.org/abs/2312.04433) | [code](https://dreamvideo-t2v.github.io/) |  Image/Video Edit |
| arxiv23.06 | [VideoComposer: Compositional Video Synthesis with Motion Controllability](https://arxiv.org/abs/2306.02018) | [code](https://github.com/ali-vilab/videocomposer) |  Image/Video Edit |
| Siggraph2023 | [Zero-shot Image-to-Image Translation](https://arxiv.org/abs/2302.03027) | [code](https://github.com/pix2pixzero/pix2pix-zero) |  Image/Video Edit |
| CVPR2023 workshop | [Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121) | [code](https://github.com/arpitbansal297/Universal-Guided-Diffusion) |  Image/Video Edit |
| CVPR2023 | [MaskSketch: Unpaired Structure-guided Masked Image Generation](https://arxiv.org/abs/2302.05496) | [code](https://github.com/google-research/masksketch) |  Image/Video Edit |
| arxiv23.02 | [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) | [code](https://github.com/Luis-kleinfeld/ControlNet) |  Image/Video Edit |
| CVPR2023 | [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800) | [code](https://github.com/timothybrooks/instruct-pix2pix) |  Image/Video Edit |
| Google | [Prompt-to-Prompt Image Editing with Cross Attention Control](https://arxiv.org/abs/2208.01626) | [code](https://github.com/google/prompt-to-prompt) |  Image/Video Edit |
| CVPR2023 | [Diffusion Models Are Real-Time Game Engines](https://arxiv.org/abs/2408.14837) | [code](https://github.com/lucidrains/gamengen-pytorch) | Image/Video Edit |
| Google-GameNGen | [Diffusion Models Are Real-Time Game Engines](https://arxiv.org/abs/2408.14837) | [code](https://github.com/lucidrains/gamengen-pytorch) | Video Engine |
| NeurIPS2024 | [RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths](https://arxiv.org/abs/2305.18295) | [code](https://raphael-painter.github.io/) | Text2Image/Video |
| zhipuAI-CogVideoX | [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer](https://arxiv.org/abs/2408.06072) | [code](https://github.com/THUDM/CogVideo) | Text2Image/Video |
| ICLR2024 | [PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/abs/2310.00426) | [code](https://pixart-alpha.github.io/) | Text2Image/Video |
| Stability-AI-SDXL | [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952) | [code](https://github.com/Stability-AI/generative-models) | Text2Image/Video |
| Google-Parti | [Scaling Autoregressive Models for Content-Rich Text-to-Image Generation](https://arxiv.org/abs/2206.10789) | [code](https://github.com/google-research/parti) | Text2Image/Video |
| Google-Imagen | [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487) | [code](https://github.com/lucidrains/imagen-pytorch) | Text2Image/Video |
| OpenAI-DALL-E-2 | [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2212.09748) | [code]([https://www.wpeebles.com/DiT.html](https://openai.com/dall-e-2)) | Text2Image/Video |
| NeurIPS2023 | [ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation](https://arxiv.org/abs/2304.05977) | [code](https://github.com/THUDM/ImageReward) | Text2Image/Video |
| NeurIPS2021 | [Cogview: Mastering text-to-image generation via transformers](https://arxiv.org/abs/2105.13290) | [code](https://github.com/THUDM/CogView) | Text2Image/Video |
| OpenAI-DALL-E | [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) | [code](https://github.com/openai/DALL-E) | Text2Image/Video |
| CVPR2022 | [High-Resolution Image Synthesis with Latent Diffusion Models](https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf) | [code](https://github.com/CompVis/latent-diffusion) | High Resolution |
| ACMMM2024 | [PEAN: A Diffusion-Based Prior-Enhanced Attention Network for Scene Text Image Super-Resolution](https://arxiv.org/abs/2311.17955) | [code](https://github.com/jdfxzzy/PEAN) | High Resolution |
| OpenAI | [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233) | [code](https://github.com/openai/guided-diffusion) | Basic |
| NeurIPS2021 workshop | [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598) | None | Basic |
| JMLR2022 | [Cascaded Diffusion Models for High Fidelity Image Generation](https://arxiv.org/pdf/2006.11239) | [code](https://cascaded-diffusion.github.io/n) | Basic |
| ICCV2023 | [Scalable Diffusion Models with Transformers](https://arxiv.org/pdf/2212.09748) | [code](https://www.wpeebles.com/DiT.html) | Basic |
| NeurIPS2021 | [Variational Diffusion Models](https://arxiv.org/pdf/2107.00630) | [code](https://github.com/google-research/vdm) | Basic |
| NeurIPS2020 | [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239) | [code](https://github.com/hojonathanho/diffusion) | Basic |


## VAE

|   |   |   |   |
| :------------: | :------------: | :------------: | :------------: |
| NeurIPS2017 | [VQVAE：Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937) | None |  |
| ICLR2014 | [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) | None |  |
| ICML2014 | [Stochastic Backpropagation and Approximate Inference in Deep Generative Models](https://arxiv.org/abs/1401.4082) | None |  |
